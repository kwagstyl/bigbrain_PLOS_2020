{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from getpass import getuser\n",
    "from distutils.dir_util import copy_tree\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import config\n",
    "import lasagne\n",
    "from lasagne.regularization import regularize_network_params\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from metrics import jaccard, accuracy, crossentropy, weighted_crossentropy\n",
    "from data_loader.cortical_layers_w_regions_kfold_val_train_test import CorticalLayersDataset\n",
    "\n",
    "from simple_model_1path import build_simple_model\n",
    "from profile_functions import profile2indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_FLOATX = config.floatX\n",
    "SAVEPATH = '/data1/users/kwagstyl/bigbrain/cortical_layers'\n",
    "LOADPATH = '/data1/users/kwagstyl/bigbrain/cortical_layers'\n",
    "WEIGHTS_PATH = LOADPATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters=64\n",
    "filter_size = [25]\n",
    "depth = 8\n",
    "data_augmentation={}\n",
    "block = 'bn_relu_conv'\n",
    "weight_decay=0.001\n",
    "num_epochs = 500\n",
    "max_patience =50\n",
    "resume=False\n",
    "learning_rate_value = 0.0005\n",
    "max_clipping=30\n",
    "prefix='training_40_'\n",
    "batch_size=[1000,1000,1]\n",
    "smooth_or_raw = 'both'\n",
    "shuffle_at_each_epoch = True\n",
    "minibatches_subset=0\n",
    "n_layers=6\n",
    "kfold=10\n",
    "val_fold=0\n",
    "test_fold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath=SAVEPATH\n",
    "loadpath=LOADPATH\n",
    "exp_name = 'simple_model'\n",
    "exp_name += '_dataset' + prefix\n",
    "exp_name += '_lrate=' + str(learning_rate_value)\n",
    "exp_name += '_fil=' + str(n_filters)\n",
    "exp_name += '_fsizes=' + str(filter_size)\n",
    "exp_name += '_depth=' + str(depth)\n",
    "exp_name += '_data=' + smooth_or_raw\n",
    "exp_name += '_decay=' + str(weight_decay)\n",
    "exp_name += '_pat=' + str(max_patience)\n",
    "exp_name += '_kfold=' + str(kfold)\n",
    "exp_name += '_val=' + str(val_fold)\n",
    "exp_name += '_test=' + str(test_fold)\n",
    "exp_name += '_batch_size=' + str(batch_size[0])\n",
    "exp_name += ('_noshuffle'+str(minibatches_subset)+'batch') if not shuffle_at_each_epoch else ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savepath : \n",
      "/data1/users/kwagstyl/bigbrain/cortical_layers/6cortical_layers/simple_model_datasettraining_40__lrate=0.0005_fil=64_fsizes=[25]_depth=8_data=both_decay=0.001_pat=50_kfold=10_val=0_test=1_batch_size=1000\n",
      "Loadpath : \n",
      "/data1/users/kwagstyl/bigbrain/cortical_layers/6cortical_layers/simple_model_datasettraining_40__lrate=0.0005_fil=64_fsizes=[25]_depth=8_data=both_decay=0.001_pat=50_kfold=10_val=0_test=1_batch_size=1000\n",
      "Saving directory : /data1/users/kwagstyl/bigbrain/cortical_layers/6cortical_layers/simple_model_datasettraining_40__lrate=0.0005_fil=64_fsizes=[25]_depth=8_data=both_decay=0.001_pat=50_kfold=10_val=0_test=1_batch_size=1000\n"
     ]
    }
   ],
   "source": [
    "dataset = str(n_layers)+'cortical_layers'\n",
    "savepath = os.path.join(savepath, dataset, exp_name)\n",
    "loadpath = os.path.join(loadpath, dataset, exp_name)\n",
    "print 'Savepath : '\n",
    "print savepath\n",
    "print 'Loadpath : '\n",
    "print loadpath\n",
    "\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "else:\n",
    "    print('\\033[93m The following folder already exists {}. '\n",
    "          'It will be overwritten in a few seconds...\\033[0m'.format(\n",
    "              savepath))\n",
    "\n",
    "print('Saving directory : ' + savepath)\n",
    "with open(os.path.join(savepath, \"config.txt\"), \"w\") as f:\n",
    "    for key, value in locals().items():\n",
    "        f.write('{} = {}\\n'.format(key, value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = T.tensor3('input_var') #n_example*nb_in_channels*ray_size\n",
    "target_var = T.ivector('target_var') #n_example*ray_size\n",
    "weight_vector = T.fvector('weight_vector')\n",
    "\n",
    "learn_step=  theano.shared(np.array(learning_rate_value, dtype=theano.config.floatX))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if smooth_or_raw =='both':\n",
    "    nb_in_channels = 2\n",
    "    use_threads = False\n",
    "else:\n",
    "    nb_in_channels = 1\n",
    "    use_threads = True\n",
    "\n",
    "train_iter = CorticalLayersDataset(\n",
    "    which_set='train',\n",
    "    prefix=prefix,\n",
    "    smooth_or_raw = smooth_or_raw,\n",
    "    batch_size=batch_size[0],\n",
    "    return_one_hot=False,\n",
    "    return_01c=False,\n",
    "    return_list=False,\n",
    "    use_threads=use_threads,\n",
    "    preload=True,\n",
    "    n_layers=n_layers,\n",
    "    kfold=kfold, # if None, kfold = number of regions (so there is one fold per region)\n",
    "    val_fold=val_fold, # it will use the first fold for validation\n",
    "    test_fold=test_fold) # this fold will not be used to train nor to validate\n",
    "\n",
    "val_iter = CorticalLayersDataset(\n",
    "    which_set='valid',\n",
    "    prefix=prefix,\n",
    "    smooth_or_raw = smooth_or_raw,\n",
    "    batch_size=batch_size[1],\n",
    "    return_one_hot=False,\n",
    "    return_01c=False,\n",
    "    return_list=False,\n",
    "    use_threads=use_threads,\n",
    "    preload=True,\n",
    "    n_layers=n_layers,\n",
    "    kfold=kfold, # if None, kfold = number of regions (so there is one fold per region)\n",
    "    val_fold=val_fold, # it will use the first fold for validation\n",
    "    test_fold=test_fold) # this fold will not be used to train nor to validate\n",
    "\n",
    "test_iter = CorticalLayersDataset(\n",
    "                                  which_set='test',\n",
    "                                  prefix=prefix,\n",
    "                                  smooth_or_raw = smooth_or_raw,\n",
    "                                  batch_size=batch_size[1],\n",
    "                                  return_one_hot=False,\n",
    "                                  return_01c=False,\n",
    "                                  return_list=False,\n",
    "                                  use_threads=use_threads,\n",
    "                                  preload=True,\n",
    "                                  n_layers=n_layers,\n",
    "                                  kfold=kfold, # if None, kfold = number of regions (so there is one fold per region)\n",
    "                                  val_fold=val_fold, # it will use the first fold for validation\n",
    "                                  test_fold=test_fold) # this fold will not be used to train nor to validate\n",
    "\n",
    "\n",
    "n_batches_train = train_iter.nbatches\n",
    "n_batches_val = val_iter.nbatches\n",
    "n_batches_test = test_iter.nbatches if test_iter is not None else 0\n",
    "n_classes = train_iter.non_void_nclasses\n",
    "void_labels = train_iter.void_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "simple_net_output, net = build_simple_model(input_var,\n",
    "                    filter_size = filter_size,\n",
    "                    n_filters = n_filters,\n",
    "                    depth = depth,\n",
    "                    block= block,\n",
    "                    nb_in_channels = nb_in_channels,\n",
    "                    n_classes = n_classes)\n",
    "\n",
    "#simple_net_output = last layer of the simple_model net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class=np.loadtxt(os.path.join(LOADPATH,'6layers_segmentation_August2018/training_cls.txt'))\n",
    "def compute_class_weights(Class):\n",
    "    #get unique labels and number of pixels of each class\n",
    "    unique, counts = np.unique(Class,return_counts=True)\n",
    "    #calculate freq(c) number of pixels per class divided by the total number of pixels in images where c is present\n",
    "    freq=counts.astype(float)/Class.size\n",
    "    return np.median(freq)/freq\n",
    "\n",
    "weights=compute_class_weights(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipping(x_train, l_train, max_clipping = 10):\n",
    "    \"\"\" Clip profiles at either end by random amounts. Augments data\"\"\"\n",
    "    k=-1\n",
    "    for p, l in zip(x_train, l_train):\n",
    "        k+=1\n",
    "        new_p=p.copy()\n",
    "        cut_top=np.random.randint(max_clipping)\n",
    "        cut_bottom=np.random.randint(max_clipping)\n",
    "        if cut_top >0:\n",
    "            p=np.delete(p,range(cut_top),axis=1)\n",
    "            l=np.delete(l,range(cut_top))\n",
    "        if cut_bottom >0:\n",
    "            l=np.delete(l,p.shape[1]-np.arange(cut_bottom))\n",
    "            p=np.delete(p,p.shape[1]-np.arange(cut_bottom),axis=1)\n",
    "            #interpolate back to full length.\n",
    "        x = np.linspace(0, len(p), x_train.shape[2])\n",
    "        new_p[0,:]=np.interp(x,range(p.shape[1]),p[0,:])\n",
    "        new_p[1,:]=np.interp(x,range(p.shape[1]),p[1,:])\n",
    "        new_label=np.round(np.interp(x,range(p.shape[1]),l))\n",
    "        x_train[k]=new_p\n",
    "        l_train[k]=new_label\n",
    "    return x_train, l_train\n",
    "\n",
    "# penalty to enforce smoothness\n",
    "def smooth_convolution(prediction, n_classes):\n",
    "    from lasagne.layers import Conv1DLayer as ConvLayer\n",
    "    from lasagne.layers import DimshuffleLayer, ReshapeLayer\n",
    "    prediction = ReshapeLayer(prediction, (-1, 200, n_classes))\n",
    "    # channels first\n",
    "    prediction = DimshuffleLayer(prediction, (0,2,1))\n",
    "\n",
    "    input_size = lasagne.layers.get_output(prediction).shape\n",
    "    # reshape to put each channel in the batch dimensions, to filter each\n",
    "    # channel independently\n",
    "    prediction = ReshapeLayer(prediction, (T.prod(input_size[0:2]),1,input_size[2]))\n",
    "\n",
    "    trans_filter = np.tile(np.array([0,-1.,1.]).astype('float32'), (1,1,1))\n",
    "    convolved = ConvLayer(prediction,\n",
    "                    num_filters = 1,\n",
    "                    filter_size = 3,\n",
    "                    stride=1,\n",
    "                    b = None,\n",
    "                    nonlinearity=None,\n",
    "                    W = trans_filter,\n",
    "                    pad='same')\n",
    "\n",
    "    # reshape back\n",
    "    \n",
    "    convolved = ReshapeLayer(convolved, input_size)\n",
    "    return convolved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Defining and compiling training functions\"\n",
    "\n",
    "convolved = smooth_convolution(simple_net_output[0], n_classes)\n",
    "\n",
    "prediction, convolved = lasagne.layers.get_output([simple_net_output[0], convolved])\n",
    "\n",
    "#prediction = lasagne.layers.get_output([simple_net_output[0]])\n",
    "#loss = categorical_crossentropy(prediction, target_var)\n",
    "#loss = loss.mean()\n",
    "\n",
    "loss = weighted_crossentropy(prediction, target_var, weight_vector)\n",
    "loss = loss.mean()\n",
    "\n",
    "\n",
    "if weight_decay > 0:\n",
    "    weightsl2 = regularize_network_params(\n",
    "        simple_net_output, lasagne.regularization.l2)\n",
    "    loss += weight_decay * weightsl2\n",
    "\n",
    "#if smooth_penalty > 0:\n",
    "#    smooth_cost = T.sum(abs(convolved), axis=(1,2))\n",
    "#   loss += smooth_penalty * smooth_cost.mean()\n",
    "\n",
    "train_acc, train_sample_acc = accuracy(prediction, target_var, void_labels)\n",
    "\n",
    "params = lasagne.layers.get_all_params(simple_net_output, trainable=True)\n",
    "updates = lasagne.updates.adam(loss, params, learning_rate=learn_step)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var, weight_vector], [loss, train_acc, train_sample_acc, prediction],\n",
    "                           updates=updates)#, profile=True)\n",
    "\n",
    "print \"Done\"\n",
    "print \"Defining and compiling valid functions\"\n",
    "valid_prediction = lasagne.layers.get_output(simple_net_output[0],\n",
    "                                            deterministic=True)\n",
    "#valid_loss = categorical_crossentropy(valid_prediction, target_var)\n",
    "#valid_loss = valid_loss.mean()\n",
    "valid_loss = weighted_crossentropy(valid_prediction, target_var, weight_vector)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "#valid_loss = crossentropy(valid_prediction, target_var, void_labels)\n",
    "valid_acc, valid_sample_acc = accuracy(valid_prediction, target_var, void_labels)\n",
    "valid_jacc = jaccard(valid_prediction, target_var, n_classes)\n",
    "\n",
    "valid_fn = theano.function([input_var, target_var, weight_vector],\n",
    "                           [valid_loss, valid_acc, valid_sample_acc, valid_jacc])#,profile=True)\n",
    "print \"Done\"\n",
    "print \"Defining and compiling test functions\"\n",
    "test_prediction = lasagne.layers.get_output(simple_net_output[0],\n",
    "                                            deterministic=True)\n",
    "test_loss = weighted_crossentropy(valid_prediction, target_var, weight_vector)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc, test_sample_acc = accuracy(test_prediction, target_var, void_labels)\n",
    "test_jacc = jaccard(test_prediction, target_var, n_classes)\n",
    "\n",
    "test_fn = theano.function([input_var, target_var, weight_vector], [test_loss, test_acc,\n",
    "                                                    test_sample_acc,test_jacc, test_prediction])\n",
    "print \"Done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.7 # for extracting the very incorrect labelled samples\n",
    "ratios=[0.80,0.85, 0.90] #ratios for the per sample accuracy\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Train loop\n",
    "#\n",
    "err_train = []\n",
    "acc_train = []\n",
    "sample_acc_train_tot = []\n",
    "worse_indices_train = []\n",
    "already_seen_idx = []\n",
    "\n",
    "err_valid = []\n",
    "acc_valid = []\n",
    "jacc_valid = []\n",
    "sample_acc_valid_tot = []\n",
    "patience = 0\n",
    "worse_indices_valid =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    #learn_step.set_value((learn_step.get_value()*0.99).astype(theano.config.floatX))\n",
    "\n",
    "    # Single epoch training and validation\n",
    "    start_time = time.time()\n",
    "    #Cost train and acc train for this epoch\n",
    "    cost_train_epoch = 0\n",
    "    acc_train_epoch = 0\n",
    "    sample_acc_train_epoch = np.array([0.0 for i in range(len(ratios))])\n",
    "    # worse_indices_train_epoch = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(n_batches_train):\n",
    "        print(i)\n",
    "        # Get minibatch (comment the next line if only 1 minibatch in training)\n",
    "        train_batch = train_iter.next()\n",
    "        X_train_batch, L_train_batch = train_batch['data'], train_batch['labels']\n",
    "   #     if max_clipping > 0:\n",
    "   #         X_train_batch, L_train_batch = clipping(X_train_batch, L_train_batch, max_clipping)\n",
    "        L_train_batch = np.reshape(L_train_batch, np.prod(L_train_batch.shape))\n",
    "        \n",
    "        # Training step\n",
    "        cost_train_batch, acc_train_batch, sample_acc_train_batch, pred = train_fn(\n",
    "                            X_train_batch, L_train_batch, weights[L_train_batch].astype('float32'))\n",
    "        sample_acc_train_batch_mean = [np.mean([(i>=ratio)\n",
    "                                for i in sample_acc_train_batch]) for ratio in ratios]\n",
    "\n",
    "        # worse_indices_train_batch = index_worse_than(sample_acc_train_batch,\n",
    "        #                                   idx_train_batch, treshold=treshold)\n",
    "\n",
    "        #print i, 'training batch cost : ', cost_train_batch, ' batch accuracy : ', acc_train_batch\n",
    "\n",
    "        #Update epoch results\n",
    "        cost_train_epoch += cost_train_batch\n",
    "        acc_train_epoch += acc_train_batch\n",
    "        sample_acc_train_epoch += sample_acc_train_batch_mean\n",
    "        # worse_indices_train_epoch = np.hstack((worse_indices_train_epoch,worse_indices_train_batch))\n",
    "\n",
    "    #Add epoch results\n",
    "    err_train += [cost_train_epoch/n_batches_train]\n",
    "    acc_train += [acc_train_epoch/n_batches_train]\n",
    "    sample_acc_train_tot += [sample_acc_train_epoch/n_batches_train]\n",
    "    # worse_indices_train += [worse_indices_train_epoch]\n",
    "\n",
    "    # Validation\n",
    "    cost_val_epoch = 0\n",
    "    acc_val_epoch = 0\n",
    "    sample_acc_valid_epoch = np.array([0.0 for i in range(len(ratios))])\n",
    "    jacc_val_epoch = np.zeros((2, n_classes))\n",
    "    # worse_indices_val_epoch = []\n",
    "\n",
    "    for i in range(n_batches_val):\n",
    "        # Get minibatch (comment the next line if only 1 minibatch in training)\n",
    "        val_batch = val_iter.next()\n",
    "        X_val_batch, L_val_batch = val_batch['data'], val_batch['labels']\n",
    "\n",
    "        L_val_batch = np.reshape(L_val_batch, np.prod(L_val_batch.shape))\n",
    "\n",
    "\n",
    "        # Validation step\n",
    "        cost_val_batch, acc_val_batch, sample_acc_valid_batch, jacc_val_batch = valid_fn(X_val_batch, L_val_batch, weights[L_val_batch].astype('float32'))\n",
    "        #print i, 'validation batch cost : ', cost_val_batch, ' batch accuracy : ', acc_val_batch\n",
    "\n",
    "\n",
    "        sample_acc_valid_batch_mean = [np.mean([(i>=ratio)\n",
    "                                for i in sample_acc_valid_batch]) for ratio in ratios]\n",
    "\n",
    "\n",
    "\n",
    "        #Update epoch results\n",
    "        cost_val_epoch += cost_val_batch\n",
    "        acc_val_epoch += acc_val_batch\n",
    "        sample_acc_valid_epoch += sample_acc_valid_batch_mean\n",
    "        jacc_val_epoch += jacc_val_batch\n",
    "        # worse_indices_val_epoch = np.hstack((worse_indices_val_epoch, worse_indices_val_batch))\n",
    "        #\n",
    "\n",
    "\n",
    "    #Add epoch results\n",
    "    err_valid += [cost_val_epoch/n_batches_val]\n",
    "    acc_valid += [acc_val_epoch/n_batches_val]\n",
    "    sample_acc_valid_tot += [sample_acc_valid_epoch/n_batches_val]\n",
    "    jacc_perclass_valid = jacc_val_epoch[0, :] / jacc_val_epoch[1, :]\n",
    "    jacc_valid += [np.mean(jacc_perclass_valid)]\n",
    "    # worse_indices_valid += [worse_indices_val_epoch]\n",
    "\n",
    "\n",
    "    #Print results (once per epoch)\n",
    "\n",
    "    out_str = \"EPOCH %i: Avg cost train %f, acc train %f\"+        \", cost val %f, acc val %f, jacc val %f took %f s\"\n",
    "    out_str = out_str % (epoch, err_train[epoch],\n",
    "                         acc_train[epoch],\n",
    "                         err_valid[epoch],\n",
    "                         acc_valid[epoch],\n",
    "                         jacc_valid[epoch],\n",
    "                         time.time()-start_time)\n",
    "    out_str2 = 'Per sample accuracy (ratios ' + str(ratios) + ') '\n",
    "    out_str2 += ' train ' +str(sample_acc_train_tot[epoch])\n",
    "    out_str2 += ' valid ' + str(sample_acc_valid_tot[epoch])\n",
    "    print out_str\n",
    "    print out_str2\n",
    "\n",
    "    with open(os.path.join(savepath, \"fcn1D_output.log\"), \"a\") as f:\n",
    "        f.write(out_str + \"\\n\")\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_jacc_val = jacc_valid[epoch]\n",
    "    elif epoch > 1 and jacc_valid[epoch] > best_jacc_val:\n",
    "        print('saving and testing best (and last) model')\n",
    "        best_jacc_val = jacc_valid[epoch]\n",
    "        patience = 0\n",
    "        np.savez(os.path.join(savepath, 'new_fcn1D_model_best.npz'),\n",
    "                 *lasagne.layers.get_all_param_values(simple_net_output))\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_errors_best.npz\"),\n",
    "                 err_train=err_train, acc_train=acc_train,\n",
    "                 err_valid=err_valid, acc_valid=acc_valid, jacc_valid=jacc_valid)\n",
    "        np.savez(os.path.join(savepath, 'new_fcn1D_model_last.npz'),\n",
    "                 *lasagne.layers.get_all_param_values(simple_net_output))\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_errors_last.npz\"),\n",
    "                 err_train=err_train, acc_train=acc_train,\n",
    "                 err_valid=err_valid, acc_valid=acc_valid, jacc_valid=jacc_valid)\n",
    "        cost_test_epoch=0\n",
    "        acc_test_epoch=0\n",
    "        sample_acc_test_epoch=0\n",
    "        jacc_test_epoch=0\n",
    "        layer_prediction=[]\n",
    "                 #test the best iteration and save results for comparisons\n",
    "        for i in range(n_batches_test):\n",
    "            test_batch = test_iter.next()\n",
    "            X_test_batch, L_test_batch = test_batch['data'], test_batch['labels']\n",
    "        \n",
    "            L_test_batch = np.reshape(L_test_batch, np.prod(L_test_batch.shape))\n",
    "        \n",
    "        # Testing step\n",
    "            cost_test_batch, acc_test_batch, sample_acc_test_batch, jacc_test_batch, test_pred = test_fn(\n",
    "                                    X_test_batch, L_test_batch, weights[L_test_batch].astype('float32'))\n",
    "            for l,p in zip(L_test_batch,test_pred):\n",
    "                layer_prediction.append(layer_prediction(p,l))\n",
    "                \n",
    "            sample_acc_test_batch_mean = [np.mean([(i>=ratio)\n",
    "                                    for i in sample_acc_test_batch]) for ratio in ratios]\n",
    "                                    \n",
    "\n",
    "                                    \n",
    "            cost_test_epoch += cost_test_batch\n",
    "            acc_test_epoch += acc_test_batch\n",
    "            sample_acc_test_epoch += sample_acc_test_batch_mean\n",
    "            jacc_test_epoch += jacc_test_batch\n",
    "        layer_prediction=np.array(layer_prediction)\n",
    "        layer_std=np.std(layer_prediction,axis=0)\n",
    "        layer_mean=np.mean(layer_prediction,axis=0)\n",
    "        err_test = [cost_test_epoch/n_batches_test]\n",
    "        acc_test = [acc_test_epoch/n_batches_test]\n",
    "        sample_acc_test_tot = [sample_acc_test_epoch/n_batches_test]\n",
    "        jacc_perclass_test = jacc_test_epoch[0, :] / jacc_test_epoch[1, :]\n",
    "        jacc_test = [np.mean(jacc_perclass_test)]\n",
    "\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_test_errors_best.npz\"),\n",
    "             err_test=err_test, acc_test=acc_test, jacc_test=jacc_test, layer_std=layer_std, layer_mean=layer_mean)\n",
    "    else:\n",
    "        patience += 1\n",
    "        print('saving last model')\n",
    "        np.savez(os.path.join(savepath, 'new_fcn1D_model_last.npz'),\n",
    "                 *lasagne.layers.get_all_param_values(simple_net_output))\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_errors_last.npz\"),\n",
    "                 err_train=err_train, acc_train=acc_train,\n",
    "                 err_valid=err_valid, acc_valid=acc_valid, jacc_valid=jacc_valid)\n",
    "    # Finish training if patience has expired or max nber of epochs reached\n",
    "\n",
    "    if patience == max_patience or epoch == num_epochs-1:\n",
    "        if savepath != loadpath:\n",
    "            print('Copying model and other training files to {}'.format(loadpath))\n",
    "            copy_tree(savepath, loadpath)\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float32, vector)>)]\n",
      "Looping 1000 times took 3.286502 seconds\n",
      "Result is [1.2317803 1.6187934 1.5227807 ... 2.2077181 2.2996776 1.6232328]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, tensor\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.22074766, dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1113976687192917"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
