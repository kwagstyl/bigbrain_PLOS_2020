{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from getpass import getuser\n",
    "from distutils.dir_util import copy_tree\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import config\n",
    "import lasagne\n",
    "from lasagne.regularization import regularize_network_params\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "from lasagne.objectives import categorical_accuracy\n",
    "\n",
    "import PIL.Image as Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import gridspec\n",
    "\n",
    "#from fcn_1D_general import buildFCN_1D\n",
    "from metrics import accuracy, crossentropy\n",
    "from cortical_layers import ParcellationDataset\n",
    "from classif_model import classif_model, dense_model\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "SAVEPATH = '/data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/NN/'\n",
    "LOADPATH = '/data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/NN/'\n",
    "WEIGHTS_PATH = LOADPATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Model hyperparameters\n",
    "n_filters = 32\n",
    "filter_size = [15]#[7,15,25,49]\n",
    "depth  = 2\n",
    "data_augmentation={} #{'horizontal_flip': True, 'fill_mode':'constant'}\n",
    "block = 'bn_relu_conv'\n",
    "\n",
    "#Training loop hyperparameters\n",
    "weight_decay=0.001\n",
    "num_epochs=500\n",
    "max_patience=100\n",
    "resume=False\n",
    "learning_rate_value = 0.0005 #learning rate is defined below as a theano variable.\n",
    "\n",
    "\n",
    "\n",
    "#Hyperparameters for the dataset loader\n",
    "batch_size=[1000,1000,1]\n",
    "smooth_or_raw = 'both'\n",
    "shuffle_at_each_epoch = True\n",
    "minibatches_subset = 0\n",
    "n_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Savepath : \n",
      "/data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/NN/parcellation/classif_model_alldataset_lrate=0.0005_fil=32_fsizes=[15]_depth=2_data=both_decay=0.001_pat=100\n",
      "Loadpath : \n",
      "/data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/NN/parcellation/classif_model_alldataset_lrate=0.0005_fil=32_fsizes=[15]_depth=2_data=both_decay=0.001_pat=100\n",
      "\u001b[93m The following folder already exists /data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/NN/parcellation/classif_model_alldataset_lrate=0.0005_fil=32_fsizes=[15]_depth=2_data=both_decay=0.001_pat=100. It will be overwritten in a few seconds...\u001b[0m\n",
      "Saving directory : /data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/NN/parcellation/classif_model_alldataset_lrate=0.0005_fil=32_fsizes=[15]_depth=2_data=both_decay=0.001_pat=100\n"
     ]
    }
   ],
   "source": [
    "_FLOATX = config.floatX\n",
    "\n",
    "savepath=SAVEPATH\n",
    "loadpath=LOADPATH\n",
    "\n",
    "exp_name = 'classif_model_alldataset'\n",
    "exp_name += '_lrate=' + str(learning_rate_value)\n",
    "exp_name += '_fil=' + str(n_filters)\n",
    "exp_name += '_fsizes=' + str(filter_size)\n",
    "exp_name += '_depth=' + str(depth)\n",
    "exp_name += '_data=' + smooth_or_raw\n",
    "exp_name += '_decay=' + str(weight_decay)\n",
    "exp_name += '_pat=' + str(max_patience)\n",
    "exp_name += ('_noshuffle'+str(minibatches_subset)+'batch') if not shuffle_at_each_epoch else ''\n",
    "#exp_name += 'test'\n",
    "\n",
    "dataset = 'parcellation'\n",
    "savepath = os.path.join(savepath, dataset, exp_name)\n",
    "loadpath = os.path.join(loadpath, dataset, exp_name)\n",
    "print 'Savepath : '\n",
    "print savepath\n",
    "print 'Loadpath : '\n",
    "print loadpath\n",
    "\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "else:\n",
    "    print('\\033[93m The following folder already exists {}. '\n",
    "          'It will be overwritten in a few seconds...\\033[0m'.format(\n",
    "              savepath))\n",
    "\n",
    "print('Saving directory : ' + savepath)\n",
    "with open(os.path.join(savepath, \"config.txt\"), \"w\") as f:\n",
    "    for key, value in locals().items():\n",
    "        f.write('{} = {}\\n'.format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input_var = T.tensor3('input_var') #n_example*nb_in_channels*ray_size\n",
    "target_var = T.ivector('target_var') #n_example*ray_size\n",
    "\n",
    "learn_step=  theano.shared(np.array(learning_rate_value, dtype=theano.config.floatX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train /data1/users/kwagstyl/bigbrain/NeuralNetworks/cortical_layers/cortical_layers/Parcellation/training_cls.txt 5184\n",
      "val /data1/users/kwagstyl/bigbrain/NeuralNetworks/cortical_layers/cortical_layers/Parcellation/training_cls.txt 5184\n",
      "nb batches train 5 valid 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build dataset iterator\n",
    "#\n",
    "\n",
    "if smooth_or_raw =='both':\n",
    "    nb_in_channels = 2\n",
    "    use_threads = False\n",
    "else:\n",
    "    nb_in_channels = 1\n",
    "    use_threads = True\n",
    "\n",
    "train_iter = ParcellationDataset(\n",
    "    which_set='train',\n",
    "    smooth_or_raw = 'both',\n",
    "    batch_size=batch_size[0],\n",
    "    data_augm_kwargs=data_augmentation,\n",
    "    shuffle_at_each_epoch = shuffle_at_each_epoch,\n",
    "    return_one_hot=False,\n",
    "    return_01c=False,\n",
    "    return_list=False,\n",
    "    use_threads=use_threads)\n",
    "\n",
    "val_iter = ParcellationDataset(\n",
    "    which_set='valid',\n",
    "    smooth_or_raw = 'both',\n",
    "    batch_size=batch_size[1],\n",
    "    shuffle_at_each_epoch = shuffle_at_each_epoch,\n",
    "    return_one_hot=False,\n",
    "    return_01c=False,\n",
    "    return_list=False,\n",
    "    use_threads=use_threads)\n",
    "\n",
    "test_iter = None\n",
    "\n",
    "n_batches_train = train_iter.nbatches\n",
    "n_batches_val = val_iter.nbatches\n",
    "n_batches_test = test_iter.nbatches if test_iter is not None else 0\n",
    "n_classes = train_iter.non_void_nclasses\n",
    "void_labels = train_iter.void_labels\n",
    "\n",
    "print 'nb batches train ' + str(n_batches_train) + ' valid ' + str(n_batches_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train /data1/users/kwagstyl/bigbrain/NeuralNetworks/cortical_layers/cortical_layers/Parcellation/training_cls.txt 5184\n"
     ]
    }
   ],
   "source": [
    "train_iter = ParcellationDataset(\n",
    "    which_set='train',\n",
    "    smooth_or_raw = 'both',\n",
    "    batch_size=batch_size[0],\n",
    "    data_augm_kwargs=data_augmentation,\n",
    "    shuffle_at_each_epoch = shuffle_at_each_epoch,\n",
    "    return_one_hot=False,\n",
    "    return_01c=False,\n",
    "    return_list=False,\n",
    "    use_threads=use_threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val /data1/users/kwagstyl/bigbrain/NeuralNetworks/cortical_layers/cortical_layers/Parcellation/training_cls.txt 5184\n"
     ]
    }
   ],
   "source": [
    "val_iter = ParcellationDataset(\n",
    "    which_set='valid',\n",
    "    smooth_or_raw = smooth_or_raw,\n",
    "    batch_size=batch_size[1],\n",
    "    shuffle_at_each_epoch = shuffle_at_each_epoch,\n",
    "    return_one_hot=False,\n",
    "    return_01c=False,\n",
    "    return_list=False,\n",
    "    use_threads=use_threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb batches train 5 valid 1\n"
     ]
    }
   ],
   "source": [
    "test_iter = None\n",
    "\n",
    "\n",
    "\n",
    "n_batches_train = train_iter.nbatches\n",
    "n_batches_val = val_iter.nbatches\n",
    "n_batches_test = test_iter.nbatches if test_iter is not None else 0\n",
    "n_classes = train_iter.non_void_nclasses\n",
    "void_labels = train_iter.void_labels\n",
    "print 'nb batches train ' + str(n_batches_train) + ' valid ' + str(n_batches_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = dense_model()\n",
    "simple_net_output, net = model.build_model(input_var, nb_in_channels = nb_in_channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and compiling training functions\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Defining and compiling training functions\"\n",
    "\n",
    "prediction = lasagne.layers.get_output(simple_net_output[0])\n",
    "loss = categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "if weight_decay > 0:\n",
    "    weightsl2 = regularize_network_params(\n",
    "        simple_net_output, lasagne.regularization.l2)\n",
    "    loss += weight_decay * weightsl2\n",
    "train_acc = T.mean(categorical_accuracy(prediction, target_var))\n",
    "\n",
    "params = lasagne.layers.get_all_params(simple_net_output, trainable=True)\n",
    "updates = lasagne.updates.adam(loss, params, learning_rate=learn_step)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], [loss, train_acc],\n",
    "                           updates=updates)\n",
    "\n",
    "print \"Done\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining and compiling valid functions\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print \"Defining and compiling valid functions\"\n",
    "valid_prediction = lasagne.layers.get_output(simple_net_output[0],\n",
    "                                            deterministic=True)\n",
    "valid_loss = categorical_crossentropy(valid_prediction, target_var)\n",
    "valid_loss = valid_loss.mean()\n",
    "\n",
    "valid_acc = T.mean(categorical_accuracy(valid_prediction, target_var))\n",
    "\n",
    "valid_fn = theano.function([input_var, target_var],\n",
    "                           [valid_loss, valid_acc])#,profile=True)\n",
    "print \"Done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "EPOCH 0: Avg cost train 221.773302, acc train 0.439789, cost val 202.445323, acc val 0.493573, took 2.059717 s\n",
      "EPOCH 1: Avg cost train 164.582016, acc train 0.516263, cost val 142.110917, acc val 0.516710, took 1.873151 s\n",
      "saving last model\n",
      "EPOCH 2: Avg cost train 106.212364, acc train 0.463960, cost val 103.903501, acc val 0.541131, took 1.872188 s\n",
      "saving best (and last) model\n",
      "EPOCH 3: Avg cost train 75.740173, acc train 0.564627, cost val 76.697940, acc val 0.438303, took 1.875944 s\n",
      "saving last model\n",
      "EPOCH 4: Avg cost train 55.947915, acc train 0.512123, cost val 66.430711, acc val 0.625964, took 2.089875 s\n",
      "saving best (and last) model\n",
      "EPOCH 5: Avg cost train 44.113902, acc train 0.593489, cost val 51.474559, acc val 0.480720, took 2.238270 s\n",
      "saving last model\n",
      "EPOCH 6: Avg cost train 33.847682, acc train 0.579723, cost val 43.233025, acc val 0.649100, took 2.268769 s\n",
      "saving best (and last) model\n",
      "EPOCH 7: Avg cost train 28.042809, acc train 0.592512, cost val 37.084904, acc val 0.656812, took 2.265839 s\n",
      "saving best (and last) model\n",
      "EPOCH 8: Avg cost train 25.590468, acc train 0.606679, cost val 35.298025, acc val 0.665810, took 2.249990 s\n",
      "saving best (and last) model\n",
      "EPOCH 9: Avg cost train 22.114812, acc train 0.613394, cost val 29.753625, acc val 0.654242, took 2.001720 s\n",
      "saving last model\n",
      "EPOCH 10: Avg cost train 18.121052, acc train 0.641238, cost val 25.932754, acc val 0.503856, took 1.886597 s\n",
      "saving last model\n",
      "EPOCH 11: Avg cost train 16.999715, acc train 0.595307, cost val 23.655413, acc val 0.503856, took 1.886325 s\n",
      "saving last model\n",
      "EPOCH 12: Avg cost train 15.592094, acc train 0.636565, cost val 22.631049, acc val 0.501285, took 2.103883 s\n",
      "saving last model\n",
      "EPOCH 13: Avg cost train 15.644140, acc train 0.592634, cost val 19.529403, acc val 0.651671, took 2.235292 s\n",
      "saving last model\n",
      "EPOCH 14: Avg cost train 13.374592, acc train 0.607346, cost val 19.561478, acc val 0.649100, took 2.246077 s\n",
      "saving last model\n",
      "EPOCH 15: Avg cost train 12.518875, acc train 0.597868, cost val 20.287914, acc val 0.658098, took 2.119606 s\n",
      "saving last model\n",
      "EPOCH 16: Avg cost train 12.999330, acc train 0.637230, cost val 19.913121, acc val 0.488432, took 2.233503 s\n",
      "saving last model\n",
      "EPOCH 17: Avg cost train 10.874603, acc train 0.627483, cost val 15.723807, acc val 0.503856, took 2.261723 s\n",
      "saving last model\n",
      "EPOCH 18: Avg cost train 8.351463, acc train 0.621938, cost val 15.227289, acc val 0.492288, took 3.089032 s\n",
      "saving last model\n",
      "EPOCH 19: Avg cost train 8.783532, acc train 0.587581, cost val 12.691464, acc val 0.654242, took 1.975242 s\n",
      "saving last model\n",
      "EPOCH 20: Avg cost train 7.119126, acc train 0.639931, cost val 12.940980, acc val 0.659383, took 1.947576 s\n",
      "saving last model\n",
      "EPOCH 21: Avg cost train 7.157399, acc train 0.609015, cost val 12.060096, acc val 0.655527, took 2.250745 s\n",
      "saving last model\n",
      "EPOCH 22: Avg cost train 6.837261, acc train 0.634479, cost val 11.774744, acc val 0.655527, took 1.875357 s\n",
      "saving last model\n",
      "EPOCH 23: Avg cost train 8.488314, acc train 0.610961, cost val 16.607315, acc val 0.650386, took 1.868894 s\n",
      "saving last model\n",
      "EPOCH 24: Avg cost train 10.581595, acc train 0.599981, cost val 10.493513, acc val 0.649100, took 2.246230 s\n",
      "saving last model\n",
      "EPOCH 25: Avg cost train 6.688814, acc train 0.685579, cost val 16.385519, acc val 0.470437, took 2.410695 s\n",
      "saving last model\n",
      "EPOCH 26: Avg cost train 9.203248, acc train 0.601931, cost val 14.012203, acc val 0.641388, took 2.347751 s\n",
      "saving last model\n",
      "EPOCH 27: Avg cost train 8.192354, acc train 0.603116, cost val 12.120042, acc val 0.655527, took 2.746053 s\n",
      "saving last model\n",
      "EPOCH 28: Avg cost train 5.918882, acc train 0.655624, cost val 9.823474, acc val 0.659383, took 2.964977 s\n",
      "saving last model\n",
      "EPOCH 29: Avg cost train 4.864035, acc train 0.649487, cost val 9.184329, acc val 0.665810, took 2.260687 s\n",
      "saving last model\n",
      "EPOCH 30: Avg cost train 5.035357, acc train 0.646746, cost val 9.167516, acc val 0.498715, took 2.248089 s\n",
      "saving last model\n",
      "EPOCH 31: Avg cost train 3.784686, acc train 0.645175, cost val 9.248294, acc val 0.488432, took 2.262423 s\n",
      "saving last model\n",
      "EPOCH 32: Avg cost train 5.371928, acc train 0.598933, cost val 8.473815, acc val 0.508997, took 2.272708 s\n",
      "saving last model\n",
      "EPOCH 33: Avg cost train 5.046691, acc train 0.621718, cost val 8.489853, acc val 0.655527, took 2.266217 s\n",
      "saving last model\n",
      "EPOCH 34: Avg cost train 4.406965, acc train 0.637620, cost val 8.336219, acc val 0.655527, took 2.274066 s\n",
      "saving last model\n",
      "EPOCH 35: Avg cost train 6.621169, acc train 0.606161, cost val 14.345647, acc val 0.659383, took 2.262077 s\n",
      "saving last model\n",
      "EPOCH 36: Avg cost train 5.929388, acc train 0.606990, cost val 11.965619, acc val 0.656812, took 2.782011 s\n",
      "saving last model\n",
      "EPOCH 37: Avg cost train 5.833803, acc train 0.606059, cost val 11.123554, acc val 0.643959, took 1.995655 s\n",
      "saving last model\n",
      "EPOCH 38: Avg cost train 7.862757, acc train 0.687649, cost val 11.261348, acc val 0.452442, took 1.874105 s\n",
      "saving last model\n",
      "EPOCH 39: Avg cost train 6.692660, acc train 0.604883, cost val 10.366279, acc val 0.651671, took 1.870373 s\n",
      "saving last model\n",
      "EPOCH 40: Avg cost train 6.171356, acc train 0.596868, cost val 14.997436, acc val 0.660668, took 2.127426 s\n",
      "saving last model\n",
      "EPOCH 41: Avg cost train 7.866320, acc train 0.662478, cost val 10.867385, acc val 0.460154, took 2.262554 s\n",
      "saving last model\n",
      "EPOCH 42: Avg cost train 4.250929, acc train 0.632494, cost val 12.666384, acc val 0.457584, took 2.265694 s\n",
      "saving last model\n",
      "EPOCH 43: Avg cost train 7.349499, acc train 0.622535, cost val 12.921586, acc val 0.652956, took 1.932038 s\n",
      "saving last model\n",
      "EPOCH 44: Avg cost train 5.965892, acc train 0.608546, cost val 10.893117, acc val 0.654242, took 1.873578 s\n",
      "saving last model\n",
      "EPOCH 45: Avg cost train 4.569679, acc train 0.649361, cost val 7.185198, acc val 0.646530, took 2.094701 s\n",
      "saving last model\n",
      "EPOCH 46: Avg cost train 4.004792, acc train 0.657687, cost val 7.452532, acc val 0.488432, took 2.239976 s\n",
      "saving last model\n",
      "EPOCH 47: Avg cost train 2.764513, acc train 0.615712, cost val 7.519203, acc val 0.654242, took 2.264329 s\n",
      "saving last model\n",
      "EPOCH 48: Avg cost train 3.143992, acc train 0.653609, cost val 7.060760, acc val 0.498715, took 2.266873 s\n",
      "saving last model\n",
      "EPOCH 49: Avg cost train 3.463232, acc train 0.611381, cost val 6.808861, acc val 0.643959, took 2.268797 s\n",
      "saving last model\n",
      "EPOCH 50: Avg cost train 3.013845, acc train 0.662198, cost val 7.676395, acc val 0.650386, took 2.269254 s\n",
      "saving last model\n",
      "EPOCH 51: Avg cost train 2.654581, acc train 0.636175, cost val 8.213630, acc val 0.474293, took 2.267769 s\n",
      "saving last model\n",
      "EPOCH 52: Avg cost train 2.839471, acc train 0.614678, cost val 8.521820, acc val 0.643959, took 2.266083 s\n",
      "saving last model\n",
      "EPOCH 53: Avg cost train 3.760647, acc train 0.653238, cost val 8.150995, acc val 0.643959, took 2.261873 s\n",
      "saving last model\n",
      "EPOCH 54: Avg cost train 3.431152, acc train 0.636315, cost val 8.306676, acc val 0.651671, took 2.265854 s\n",
      "saving last model\n",
      "EPOCH 55: Avg cost train 3.728005, acc train 0.623848, cost val 9.999396, acc val 0.655527, took 2.160105 s\n",
      "saving last model\n",
      "EPOCH 56: Avg cost train 5.380341, acc train 0.701468, cost val 15.765134, acc val 0.422879, took 1.873571 s\n",
      "saving last model\n",
      "EPOCH 57: Avg cost train 6.933875, acc train 0.603350, cost val 8.418367, acc val 0.655527, took 1.870970 s\n",
      "saving last model\n",
      "EPOCH 58: Avg cost train 2.696808, acc train 0.609985, cost val 8.229998, acc val 0.650386, took 1.873391 s\n",
      "saving last model\n",
      "EPOCH 59: Avg cost train 3.446642, acc train 0.659901, cost val 6.174573, acc val 0.654242, took 2.244097 s\n",
      "saving last model\n",
      "EPOCH 60: Avg cost train 2.671469, acc train 0.671842, cost val 9.786180, acc val 0.458869, took 2.263933 s\n",
      "saving last model\n",
      "EPOCH 61: Avg cost train 3.552910, acc train 0.654761, cost val 8.993343, acc val 0.466581, took 2.196308 s\n",
      "saving last model\n",
      "EPOCH 62: Avg cost train 5.007171, acc train 0.631579, cost val 8.808173, acc val 0.655527, took 3.060174 s\n",
      "saving last model\n",
      "EPOCH 63: Avg cost train 3.633431, acc train 0.630072, cost val 8.487792, acc val 0.651671, took 2.272955 s\n",
      "saving last model\n",
      "EPOCH 64: Avg cost train 4.330390, acc train 0.619446, cost val 12.646248, acc val 0.659383, took 2.276790 s\n",
      "saving last model\n",
      "EPOCH 65: Avg cost train 6.588066, acc train 0.663167, cost val 10.922830, acc val 0.444730, took 2.266147 s\n",
      "saving last model\n",
      "EPOCH 66: Avg cost train 4.228674, acc train 0.664024, cost val 9.636495, acc val 0.458869, took 2.094382 s\n",
      "saving last model\n",
      "EPOCH 67: Avg cost train 5.360736, acc train 0.620438, cost val 8.885775, acc val 0.640103, took 2.227821 s\n",
      "saving last model\n",
      "EPOCH 68: Avg cost train 3.772046, acc train 0.623409, cost val 8.266716, acc val 0.652956, took 2.271902 s\n",
      "saving last model\n",
      "EPOCH 69: Avg cost train 4.361533, acc train 0.631676, cost val 11.750384, acc val 0.663239, took 2.264312 s\n",
      "saving last model\n",
      "EPOCH 70: Avg cost train 6.427079, acc train 0.645900, cost val 10.711359, acc val 0.430591, took 2.275677 s\n",
      "saving last model\n",
      "EPOCH 71: Avg cost train 5.295329, acc train 0.659657, cost val 5.582776, acc val 0.647815, took 2.263190 s\n",
      "saving last model\n",
      "EPOCH 72: Avg cost train 5.107337, acc train 0.618613, cost val 11.072997, acc val 0.663239, took 2.260088 s\n",
      "saving last model\n",
      "EPOCH 73: Avg cost train 5.305868, acc train 0.615507, cost val 7.396770, acc val 0.655527, took 2.261924 s\n",
      "saving last model\n",
      "EPOCH 74: Avg cost train 3.986523, acc train 0.675156, cost val 6.219492, acc val 0.473008, took 2.260441 s\n",
      "saving last model\n",
      "EPOCH 75: Avg cost train 4.019947, acc train 0.660453, cost val 9.401944, acc val 0.446015, took 2.263083 s\n",
      "saving last model\n",
      "EPOCH 76: Avg cost train 5.291028, acc train 0.614946, cost val 8.303038, acc val 0.661954, took 2.263843 s\n",
      "saving last model\n",
      "EPOCH 77: Avg cost train 3.286703, acc train 0.635087, cost val 8.383612, acc val 0.659383, took 2.190065 s\n",
      "saving last model\n",
      "EPOCH 78: Avg cost train 3.643243, acc train 0.625828, cost val 11.086055, acc val 0.667095, took 3.092983 s\n",
      "saving best (and last) model\n",
      "EPOCH 79: Avg cost train 5.790036, acc train 0.652129, cost val 12.013254, acc val 0.435733, took 3.079898 s\n",
      "saving last model\n",
      "EPOCH 80: Avg cost train 4.938943, acc train 0.660165, cost val 5.688863, acc val 0.652956, took 1.870026 s\n",
      "saving last model\n",
      "EPOCH 81: Avg cost train 2.290034, acc train 0.665087, cost val 7.366869, acc val 0.466581, took 1.867872 s\n",
      "saving last model\n",
      "EPOCH 82: Avg cost train 2.002224, acc train 0.642165, cost val 5.543813, acc val 0.663239, took 1.949263 s\n",
      "saving last model\n",
      "EPOCH 83: Avg cost train 1.596938, acc train 0.669356, cost val 7.538835, acc val 0.663239, took 2.947118 s\n",
      "saving last model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-bc9e40fd3d2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Get minibatch (comment the next line if only 1 minibatch in training)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mtrain_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mX_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_train_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'indices'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data1/users/kwagstyl/anaconda2/lib/python2.7/site-packages/dataset_loaders-1.0.0-py2.7.egg/dataset_loaders/parallel_loader.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data1/users/kwagstyl/anaconda2/lib/python2.7/site-packages/dataset_loaders-1.0.0-py2.7.egg/dataset_loaders/parallel_loader.pyc\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m                     \u001b[0mname_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m                     \u001b[0mdata_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m                     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data1/users/kwagstyl/bigbrain/NeuralNetworks/Parcellation/cortical_layers/data_loader/parallel_loader_1D.pyc\u001b[0m in \u001b[0;36mfetch_from_dataset\u001b[1;34m(self, batch_to_load)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_to_load\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m                         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "err_train = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "\n",
    "err_valid = []\n",
    "acc_valid = []\n",
    "\n",
    "\n",
    "patience = 10\n",
    "# Training main loop\n",
    "print \"Start training\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #learn_step.set_value((learn_step.get_value()*0.99).astype(theano.config.floatX))\n",
    "\n",
    "    # Single epoch training and validation\n",
    "    start_time = time.time()\n",
    "    #Cost train and acc train for this epoch\n",
    "    cost_train_epoch = 0\n",
    "    acc_train_epoch = 0\n",
    "\n",
    "\n",
    "    for i in range(n_batches_train):\n",
    "        # Get minibatch (comment the next line if only 1 minibatch in training)\n",
    "        train_batch = train_iter.next()\n",
    "        X_train_batch, L_train_batch, idx_train_batch = train_batch['data'], train_batch['labels'], train_batch['indices'][0]\n",
    "\n",
    "\n",
    "        L_train_batch = np.reshape(L_train_batch, np.prod(L_train_batch.shape))\n",
    "\n",
    "\n",
    "        cost_train_batch, acc_train_batch = train_fn(X_train_batch, L_train_batch)\n",
    "\n",
    "        #Update epoch results\n",
    "        cost_train_epoch += cost_train_batch\n",
    "        acc_train_epoch += acc_train_batch\n",
    "\n",
    "    #Add epoch results\n",
    "    err_train += [cost_train_epoch/n_batches_train]\n",
    "    acc_train += [acc_train_epoch/n_batches_train]\n",
    "\n",
    "    # Validation\n",
    "    cost_val_epoch = 0\n",
    "    acc_val_epoch = 0\n",
    "\n",
    "    for i in range(n_batches_val):\n",
    "\n",
    "        # Get minibatch (comment the next line if only 1 minibatch in training)\n",
    "        val_batch = val_iter.next()\n",
    "        X_val_batch, L_val_batch, idx_val_batch = val_batch['data'], val_batch['labels'], val_batch['indices'][0]\n",
    "\n",
    "        L_val_batch = np.reshape(L_val_batch, np.prod(L_val_batch.shape))\n",
    "\n",
    "\n",
    "        # Validation step\n",
    "        cost_val_batch, acc_val_batch = valid_fn(X_val_batch, L_val_batch)\n",
    "        #print i, 'validation batch cost : ', cost_val_batch, ' batch accuracy : ', acc_val_batch\n",
    "\n",
    "        #Update epoch results\n",
    "        cost_val_epoch += cost_val_batch\n",
    "        acc_val_epoch += acc_val_batch\n",
    "\n",
    "\n",
    "\n",
    "    #Add epoch results\n",
    "    err_valid += [cost_val_epoch/n_batches_val]\n",
    "    acc_valid += [acc_val_epoch/n_batches_val]\n",
    "\n",
    "    #Print results (once per epoch)\n",
    "\n",
    "    out_str = \"EPOCH %i: Avg cost train %f, acc train %f, cost val %f, acc val %f, took %f s\"\n",
    "    out_str = out_str % (epoch, err_train[epoch],\n",
    "                         acc_train[epoch],\n",
    "                         err_valid[epoch],\n",
    "                         acc_valid[epoch],\n",
    "                         time.time()-start_time)\n",
    "    print out_str\n",
    "    # print out_str2\n",
    "\n",
    "\n",
    "\n",
    "    # Early stopping and saving stuff\n",
    "\n",
    "    with open(os.path.join(savepath, \"fcn1D_output.log\"), \"a\") as f:\n",
    "        f.write(out_str + \"\\n\")\n",
    "\n",
    "    if epoch == 0:        \n",
    "        best_acc_val = acc_valid[epoch]\n",
    "    elif epoch > 1 and acc_valid[epoch] > best_acc_val:\n",
    "        print('saving best (and last) model')\n",
    "        best_acc_val = acc_valid[epoch]\n",
    "        patience = 0\n",
    "        np.savez(os.path.join(savepath, 'new_fcn1D_model_best.npz'),\n",
    "                 *lasagne.layers.get_all_param_values(simple_net_output))\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_errors_best.npz\"),\n",
    "                 err_train=err_train, acc_train=acc_train,\n",
    "                 err_valid=err_valid, acc_valid=acc_valid)\n",
    "        np.savez(os.path.join(savepath, 'new_fcn1D_model_last.npz'),\n",
    "                 *lasagne.layers.get_all_param_values(simple_net_output))\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_errors_last.npz\"),\n",
    "                 err_train=err_train, acc_train=acc_train,\n",
    "                 err_valid=err_valid, acc_valid=acc_valid)\n",
    "    else:\n",
    "        patience += 1\n",
    "        print('saving last model')\n",
    "        np.savez(os.path.join(savepath, 'new_fcn1D_model_last.npz'),\n",
    "                 *lasagne.layers.get_all_param_values(simple_net_output))\n",
    "        np.savez(os.path.join(savepath , \"fcn1D_errors_last.npz\"),\n",
    "                 err_train=err_train, acc_train=acc_train,\n",
    "                 err_valid=err_valid, acc_valid=acc_valid)\n",
    "    # Finish training if patience has expired or max nber of epochs reached\n",
    "\n",
    "    if patience == max_patience or epoch == num_epochs-1:\n",
    "        if savepath != loadpath:\n",
    "            print('Copying model and other training files to {}'.format(loadpath))\n",
    "            copy_tree(savepath, loadpath)\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc820675e10>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBGRThQOIiLSicJBREQ6+f+qzx4EPQ3OxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8206e0950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train_batch[670][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train_batch[670]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
